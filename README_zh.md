<h1 align="center">flexllm</h1>

<p align="center">
    <strong>ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œæ‰€æœ‰å¤§æ¨¡å‹</strong><br>
    <em>ç”Ÿäº§çº§ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€å“åº”ç¼“å­˜ã€å¤š Provider ç»Ÿä¸€æ¥å£</em>
</p>

<p align="center">
    <a href="https://pypi.org/project/flexllm/">
        <img src="https://img.shields.io/pypi/v/flexllm?color=brightgreen&style=flat-square" alt="PyPI version">
    </a>
    <a href="https://github.com/KenyonY/flexllm/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/github/license/KenyonY/flexllm.svg?color=blue&style=flat-square">
    </a>
</p>

<p align="center">
    <a href="README.md">English</a> | ä¸­æ–‡
</p>

---

## è®¾è®¡ç†å¿µ

**ä¸€ä¸ªç»Ÿä¸€å…¥å£ï¼Œé€‚é…æ‰€æœ‰ LLM æœåŠ¡å•†ã€‚**

```python
from flexllm import LLMClient

# åªéœ€å¯¼å…¥è¿™ä¸€ä¸ªç±»ï¼Œå…¶ä»–éƒ½æ˜¯é…ç½®ã€‚
```

flexllm éµå¾ª **"å•ä¸€æ¥å£ï¼Œå¤šåç«¯"** åŸåˆ™ã€‚æ— è®ºè°ƒç”¨ OpenAIã€Geminiã€Claude è¿˜æ˜¯è‡ªå»ºæ¨¡å‹ï¼ŒAPI å®Œå…¨ä¸€è‡´ã€‚Provider å·®å¼‚è¢«æŠ½è±¡å°è£…ï¼Œä½ åªéœ€å…³æ³¨ä¸šåŠ¡é€»è¾‘ã€‚

```python
# OpenAI GPT-4
client = LLMClient(base_url="https://api.openai.com/v1", model="gpt-4", api_key="...")

# Google Gemini
client = LLMClient(provider="gemini", model="gemini-2.0-flash", api_key="...")

# Anthropic Claude
client = LLMClient(provider="claude", model="claude-sonnet-4-20250514", api_key="...")

# è‡ªå»ºæœåŠ¡ (vLLM, Ollama ç­‰)
client = LLMClient(base_url="http://localhost:8000/v1", model="qwen2.5")

# API å®Œå…¨ä¸€è‡´ï¼š
result = await client.chat_completions(messages)
results = await client.chat_completions_batch(messages_list)
```

---

## æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç»Ÿä¸€æ¥å£** | ä¸€ä¸ª `LLMClient` é€‚é… OpenAIã€Geminiã€Claude åŠæ‰€æœ‰ OpenAI å…¼å®¹ API |
| **æ–­ç‚¹ç»­ä¼ ** | æ‰¹é‡ä»»åŠ¡è‡ªåŠ¨æ¢å¤ï¼Œç™¾ä¸‡çº§è¯·æ±‚å®‰å…¨å¤„ç† |
| **å“åº”ç¼“å­˜** | å†…ç½®ç¼“å­˜ï¼Œæ”¯æŒ TTL å’Œ IPC å¤šè¿›ç¨‹å…±äº« |
| **æˆæœ¬è¿½è¸ª** | å®æ—¶æˆæœ¬ç›‘æ§ï¼Œæ”¯æŒé¢„ç®—æ§åˆ¶ |
| **é«˜æ€§èƒ½å¼‚æ­¥** | ç²¾ç»†å¹¶å‘æ§åˆ¶ã€QPS é™æµã€æµå¼å¤„ç† |
| **è´Ÿè½½å‡è¡¡** | å¤š Endpoint åˆ†å‘ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§» |

---

## å®‰è£…

```bash
pip install flexllm

# å®Œæ•´åŠŸèƒ½
pip install flexllm[all]
```

---

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
from flexllm import LLMClient

client = LLMClient(
    model="gpt-4",
    base_url="https://api.openai.com/v1",
    api_key="your-api-key"
)

# å¼‚æ­¥
response = await client.chat_completions([
    {"role": "user", "content": "ä½ å¥½ï¼"}
])

# åŒæ­¥
response = client.chat_completions_sync([
    {"role": "user", "content": "ä½ å¥½ï¼"}
])
```

### æ‰¹é‡å¤„ç† + æ–­ç‚¹ç»­ä¼ 

å®‰å…¨å¤„ç†ç™¾ä¸‡çº§è¯·æ±‚ã€‚ä¸­æ–­åé‡å¯ï¼Œè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­ã€‚

```python
messages_list = [
    [{"role": "user", "content": f"é—®é¢˜ {i}"}]
    for i in range(100000)
]

# 50000 æ¡æ—¶ä¸­æ–­ï¼Ÿé‡æ–°è¿è¡Œï¼Œä» 50001 ç»§ç»­
results = await client.chat_completions_batch(
    messages_list,
    output_jsonl="results.jsonl",  # è¿›åº¦ä¿å­˜åœ¨æ­¤
    show_progress=True,
)
```

### å“åº”ç¼“å­˜

```python
from flexllm import LLMClient, ResponseCacheConfig

client = LLMClient(
    model="gpt-4",
    base_url="https://api.openai.com/v1",
    api_key="your-api-key",
    cache=ResponseCacheConfig(enabled=True, ttl=3600),  # 1å°æ—¶ TTL
)

# é¦–æ¬¡è°ƒç”¨ï¼šAPI è¯·æ±‚ (~2ç§’, ~$0.01)
result1 = await client.chat_completions(messages)

# å†æ¬¡è°ƒç”¨ï¼šç¼“å­˜å‘½ä¸­ (~0.001ç§’, $0)
result2 = await client.chat_completions(messages)
```

### æˆæœ¬è¿½è¸ª

```python
# æ‰¹é‡å¤„ç†æ—¶è¿½è¸ªæˆæœ¬
results, cost_report = await client.chat_completions_batch(
    messages_list,
    return_cost_report=True,
)
print(f"æ€»æˆæœ¬: ${cost_report.total_cost:.4f}")

# è¿›åº¦æ¡å®æ—¶æ˜¾ç¤ºæˆæœ¬
results = await client.chat_completions_batch(
    messages_list,
    track_cost=True,  # è¿›åº¦æ¡æ˜¾ç¤º ğŸ’° $0.0012
)
```

### è´Ÿè½½å‡è¡¡

```python
from flexllm import LLMClientPool

pool = LLMClientPool(
    endpoints=[
        {"base_url": "http://gpu1:8000/v1", "model": "qwen"},
        {"base_url": "http://gpu2:8000/v1", "model": "qwen"},
    ],
    load_balance="round_robin",  # æˆ– "weighted", "random", "fallback"
    fallback=True,               # æ•…éšœè‡ªåŠ¨åˆ‡æ¢
)

# è¯·æ±‚è‡ªåŠ¨åˆ†å‘
results = await pool.chat_completions_batch(messages_list, distribute=True)
```

---

## CLI

```bash
# å¿«é€Ÿé—®ç­”
flexllm ask "Python æ˜¯ä»€ä¹ˆï¼Ÿ"

# äº¤äº’å¯¹è¯
flexllm chat

# æ‰¹é‡å¤„ç† + æˆæœ¬è¿½è¸ª
flexllm batch input.jsonl -o output.jsonl --track-cost

# æ¨¡å‹ç®¡ç†
flexllm list        # å·²é…ç½®æ¨¡å‹
flexllm models      # è¿œç¨‹å¯ç”¨æ¨¡å‹
flexllm test        # æµ‹è¯•è¿æ¥
```

---

## æ¶æ„

```
flexllm/
â”œâ”€â”€ clients/           # æ‰€æœ‰å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ base.py        # æŠ½è±¡åŸºç±» (LLMClientBase)
â”‚   â”œâ”€â”€ llm.py         # ç»Ÿä¸€å…¥å£ (LLMClient)
â”‚   â”œâ”€â”€ openai.py      # OpenAI å…¼å®¹åç«¯
â”‚   â”œâ”€â”€ gemini.py      # Google Gemini åç«¯
â”‚   â”œâ”€â”€ claude.py      # Anthropic Claude åç«¯
â”‚   â”œâ”€â”€ pool.py        # å¤š Endpoint è´Ÿè½½å‡è¡¡
â”‚   â””â”€â”€ router.py      # Provider è·¯ç”±ç­–ç•¥
â”œâ”€â”€ pricing/           # æˆæœ¬ä¼°ç®—å’Œè¿½è¸ª
â”œâ”€â”€ cache/             # å“åº”ç¼“å­˜ (æ”¯æŒ IPC)
â”œâ”€â”€ async_api/         # é«˜æ€§èƒ½å¼‚æ­¥å¼•æ“
â””â”€â”€ msg_processors/    # å¤šæ¨¡æ€æ¶ˆæ¯å¤„ç†
```

åˆ†å±‚è®¾è®¡ï¼š

```
LLMClient (ç»Ÿä¸€å…¥å£ - æ¨èä½¿ç”¨)
    â”‚
    â”œâ”€â”€ Provider è‡ªåŠ¨è¯†åˆ«æˆ–æ˜¾å¼æŒ‡å®š
    â”‚
    â””â”€â”€ åç«¯å®¢æˆ·ç«¯ (å†…éƒ¨)
            â”œâ”€â”€ OpenAIClient
            â”œâ”€â”€ GeminiClient
            â””â”€â”€ ClaudeClient
                    â”‚
                    â””â”€â”€ LLMClientBase (æŠ½è±¡åŸºç±» - åªéœ€å®ç°4ä¸ªæ–¹æ³•)
                            â”‚
                            â”œâ”€â”€ ConcurrentRequester (å¼‚æ­¥å¼•æ“)
                            â”œâ”€â”€ ResponseCache (ç¼“å­˜å±‚)
                            â””â”€â”€ CostTracker (æˆæœ¬ç›‘æ§)
```

---

## è®¸å¯è¯

[Apache 2.0](LICENSE)
