<h1 align="center">flexllm</h1>

<p align="center">
    <strong>ç”Ÿäº§çº§é«˜æ€§èƒ½ LLM å®¢æˆ·ç«¯</strong><br>
    <em>æ‰¹é‡å¤„ç† + æ–­ç‚¹ç»­ä¼ ã€å“åº”ç¼“å­˜ã€è´Ÿè½½å‡è¡¡ã€æˆæœ¬è¿½è¸ª</em>
</p>

<p align="center">
    <a href="https://pypi.org/project/flexllm/">
        <img src="https://img.shields.io/pypi/v/flexllm?color=brightgreen&style=flat-square" alt="PyPI version">
    </a>
    <a href="https://github.com/KenyonY/flexllm/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/github/license/KenyonY/flexllm.svg?color=blue&style=flat-square">
    </a>
</p>

<p align="center">
    <a href="README.md">English</a> | ä¸­æ–‡
</p>

---

## ä¸ºä»€ä¹ˆé€‰æ‹© flexllmï¼Ÿ

**ä¸“ä¸ºå¤§è§„æ¨¡ç”Ÿäº§çº§æ‰¹é‡å¤„ç†è€Œè®¾è®¡ã€‚**

```python
from flexllm import LLMClient

client = LLMClient(base_url="https://api.openai.com/v1", model="gpt-4", api_key="...")

# å¤„ç† 10 ä¸‡æ¡è¯·æ±‚ï¼Œæ”¯æŒè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ 
# 50000 æ¡æ—¶ä¸­æ–­ï¼Ÿé‡æ–°è¿è¡Œï¼Œä» 50001 ç»§ç»­
results = await client.chat_completions_batch(
    messages_list,
    output_jsonl="results.jsonl",  # è¿›åº¦ä¿å­˜åœ¨æ­¤
    show_progress=True,
    track_cost=True,  # å®æ—¶æ˜¾ç¤ºæˆæœ¬
)
```

---

## æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ–­ç‚¹ç»­ä¼ ** | æ‰¹é‡ä»»åŠ¡è‡ªåŠ¨æ¢å¤ï¼Œç™¾ä¸‡çº§è¯·æ±‚å®‰å…¨å¤„ç† |
| **å“åº”ç¼“å­˜** | å†…ç½®ç¼“å­˜ï¼Œæ”¯æŒ TTL å’Œ IPC å¤šè¿›ç¨‹å…±äº« |
| **è´Ÿè½½å‡è¡¡** | å¤š Endpoint åŠ¨æ€åˆ†å‘ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§» |
| **æˆæœ¬è¿½è¸ª** | å®æ—¶æˆæœ¬ç›‘æ§ï¼Œæ”¯æŒé¢„ç®—æ§åˆ¶ |
| **é«˜æ€§èƒ½å¼‚æ­¥** | ç²¾ç»†å¹¶å‘æ§åˆ¶ã€QPS é™æµã€æµå¼å¤„ç† |
| **å¤š Provider** | æ”¯æŒ OpenAI å…¼å®¹ APIã€Geminiã€Claude |

---

## å®‰è£…

```bash
pip install flexllm

# å®Œæ•´åŠŸèƒ½
pip install flexllm[all]
```

### Claude Code é›†æˆ

è®© Claude Code å­¦ä¼šä½¿ç”¨ flexllm è¿›è¡Œ LLM API è°ƒç”¨ã€æ‰¹é‡å¤„ç†ç­‰æ“ä½œï¼š

```bash
flexllm install-skill
```

å®‰è£…åï¼ŒClaude Code åœ¨ä»»ä½•é¡¹ç›®ä¸­éƒ½èƒ½ä½¿ç”¨ flexllmã€‚

---

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
from flexllm import LLMClient

# æ¨èï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†èµ„æº
async with LLMClient(
    model="gpt-4",
    base_url="https://api.openai.com/v1",
    api_key="your-api-key"
) as client:
    # å¼‚æ­¥è°ƒç”¨
    response = await client.chat_completions([
        {"role": "user", "content": "ä½ å¥½ï¼"}
    ])

# åŒæ­¥ç‰ˆæœ¬ï¼ˆåŒæ ·æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
with LLMClient(model="gpt-4", base_url="...", api_key="...") as client:
    response = client.chat_completions_sync([
        {"role": "user", "content": "ä½ å¥½ï¼"}
    ])

# è·å– token ç”¨é‡
result = await client.chat_completions(
    messages=[{"role": "user", "content": "ä½ å¥½ï¼"}],
    return_usage=True,  # è¿”å›åŒ…å« usage ä¿¡æ¯çš„ ChatCompletionResult
)
print(f"Token ç”¨é‡: {result.usage}")  # {'prompt_tokens': 10, 'completion_tokens': 5, ...}
```

### æ‰¹é‡å¤„ç† + æ–­ç‚¹ç»­ä¼ 

å®‰å…¨å¤„ç†ç™¾ä¸‡çº§è¯·æ±‚ã€‚ä¸­æ–­åé‡å¯ï¼Œè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­ã€‚

```python
messages_list = [
    [{"role": "user", "content": f"é—®é¢˜ {i}"}]
    for i in range(100000)
]

# 50000 æ¡æ—¶ä¸­æ–­ï¼Ÿé‡æ–°è¿è¡Œï¼Œä» 50001 ç»§ç»­
results = await client.chat_completions_batch(
    messages_list,
    output_jsonl="results.jsonl",  # è¿›åº¦ä¿å­˜åœ¨æ­¤
    show_progress=True,
)
```

### å“åº”ç¼“å­˜

```python
from flexllm import LLMClient, ResponseCacheConfig

client = LLMClient(
    model="gpt-4",
    base_url="https://api.openai.com/v1",
    api_key="your-api-key",
    cache=ResponseCacheConfig(enabled=True, ttl=3600),  # 1å°æ—¶ TTL
)

# é¦–æ¬¡è°ƒç”¨ï¼šAPI è¯·æ±‚ (~2ç§’, ~$0.01)
result1 = await client.chat_completions(messages)

# å†æ¬¡è°ƒç”¨ï¼šç¼“å­˜å‘½ä¸­ (~0.001ç§’, $0)
result2 = await client.chat_completions(messages)
```

### æˆæœ¬è¿½è¸ª

```python
# æ‰¹é‡å¤„ç†æ—¶è¿½è¸ªæˆæœ¬
results, cost_report = await client.chat_completions_batch(
    messages_list,
    return_cost_report=True,
)
print(f"æ€»æˆæœ¬: ${cost_report.total_cost:.4f}")

# è¿›åº¦æ¡å®æ—¶æ˜¾ç¤ºæˆæœ¬
results = await client.chat_completions_batch(
    messages_list,
    track_cost=True,  # è¿›åº¦æ¡æ˜¾ç¤º ğŸ’° $0.0012
)
```

### æµå¼è¾“å‡º

```python
# é€ token æµå¼è¾“å‡º
async for chunk in client.chat_completions_stream(messages):
    print(chunk, end="", flush=True)

# æ‰¹é‡æµå¼ - ç»“æœå®Œæˆå³è¿”å›
async for result in client.iter_chat_completions_batch(messages_list):
    process(result)
```

### æ€è€ƒæ¨¡å¼ï¼ˆæ¨ç†æ¨¡å‹ï¼‰

ç»Ÿä¸€æ¥å£æ”¯æŒ DeepSeek-R1ã€Qwen3ã€Claude æ‰©å±•æ€è€ƒã€Gemini æ€è€ƒæ¨¡å¼ã€‚

```python
result = await client.chat_completions(
    messages,
    thinking=True,      # å¯ç”¨æ€è€ƒ
    return_raw=True,
)

# è·¨ Provider ç»Ÿä¸€è§£æ
parsed = client.parse_thoughts(result.data)
print("æ€è€ƒè¿‡ç¨‹:", parsed["thought"])
print("ç­”æ¡ˆ:", parsed["answer"])
```

### å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

```python
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "è·å–å¤©æ°”ä¿¡æ¯",
        "parameters": {
            "type": "object",
            "properties": {"location": {"type": "string"}},
            "required": ["location"],
        },
    },
}]

result = await client.chat_completions(
    messages=[{"role": "user", "content": "ä¸œäº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
    tools=tools,
    return_usage=True,
)

if result.tool_calls:
    for call in result.tool_calls:
        print(f"è°ƒç”¨: {call.function['name']}({call.function['arguments']})")
```

### è´Ÿè½½å‡è¡¡ï¼ˆLLMClientPoolï¼‰

å¤š Endpoint è´Ÿè½½å‡è¡¡ï¼Œæ”¯æŒè‡ªåŠ¨æ•…éšœè½¬ç§»ã€å¥åº·æ£€æŸ¥å’ŒåŠ¨æ€ä»»åŠ¡åˆ†é…ã€‚

```python
from flexllm import LLMClientPool

pool = LLMClientPool(
    endpoints=[
        # æ¯ä¸ª endpoint å¯ç‹¬ç«‹é…ç½®é™æµå‚æ•°
        {"base_url": "http://gpu1:8000/v1", "model": "qwen", "concurrency_limit": 50, "max_qps": 100},
        {"base_url": "http://gpu2:8000/v1", "model": "qwen", "concurrency_limit": 20, "max_qps": 50},
        {"base_url": "http://gpu3:8000/v1", "model": "qwen", "weight": 2.0},  # æ›´é«˜æƒé‡ = æ›´å¤šæµé‡
    ],
    load_balance="round_robin",  # "round_robin" | "weighted" | "random" | "fallback"
    fallback=True,               # endpoint æ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢
    failure_threshold=3,         # è¿ç»­å¤±è´¥ 3 æ¬¡åæ ‡è®°ä¸ºä¸å¥åº·
    recovery_time=60.0,          # 60 ç§’åå°è¯•æ¢å¤
)

# å•æ¬¡è¯·æ±‚ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§»
result = await pool.chat_completions(messages)

# æ‰¹é‡å¤„ç†ï¼ŒåŠ¨æ€è´Ÿè½½å‡è¡¡
# å¿«çš„ endpoint è‡ªåŠ¨å¤„ç†æ›´å¤šä»»åŠ¡ï¼ˆå…±äº«é˜Ÿåˆ—æ¨¡å‹ï¼‰
results = await pool.chat_completions_batch(
    messages_list,
    distribute=True,      # å¯ç”¨åˆ†å¸ƒå¼å¤„ç†
    output_jsonl="results.jsonl",  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    track_cost=True,
)

# æµå¼è¾“å‡ºï¼Œæ”¯æŒæ•…éšœè½¬ç§»
async for chunk in pool.chat_completions_stream(messages):
    print(chunk, end="", flush=True)

# æŸ¥çœ‹æ± ç»Ÿè®¡ä¿¡æ¯
print(pool.stats)  # {'num_endpoints': 3, 'router_stats': {...}}
```

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **åŠ¨æ€è´Ÿè½½å‡è¡¡**ï¼šå…±äº«é˜Ÿåˆ—æ¨¡å‹ï¼Œå¿«çš„ endpoint è‡ªåŠ¨å¤„ç†æ›´å¤šä»»åŠ¡
- **è‡ªåŠ¨æ•…éšœè½¬ç§»**ï¼šå¤±è´¥è¯·æ±‚è‡ªåŠ¨åœ¨å…¶ä»–å¥åº· endpoint é‡è¯•
- **å¥åº·ç›‘æ§**ï¼šä¸å¥åº·çš„ endpoint åœ¨ `recovery_time` åè‡ªåŠ¨æ¢å¤
- **ç‹¬ç«‹é…ç½®**ï¼šæ¯ä¸ª endpoint å¯ç‹¬ç«‹è®¾ç½® `concurrency_limit`ã€`max_qps`ã€`weight`
- **å®Œæ•´åŠŸèƒ½æ”¯æŒ**ï¼šæ–­ç‚¹ç»­ä¼ ã€å“åº”ç¼“å­˜ã€æˆæœ¬è¿½è¸ªå‡å¯åœ¨ Pool ä¸­ä½¿ç”¨

---

## CLI

```bash
# å¿«é€Ÿé—®ç­”
flexllm ask "Python æ˜¯ä»€ä¹ˆï¼Ÿ"

# äº¤äº’å¯¹è¯
flexllm chat

# æ‰¹é‡å¤„ç† + æˆæœ¬è¿½è¸ª
flexllm batch input.jsonl -o output.jsonl --track-cost

# æ¨¡å‹ç®¡ç†
flexllm list              # å·²é…ç½®æ¨¡å‹
flexllm models            # è¿œç¨‹å¯ç”¨æ¨¡å‹
flexllm set-model gpt-4   # è®¾ç½®é»˜è®¤æ¨¡å‹
flexllm test              # æµ‹è¯•è¿æ¥
flexllm init              # åˆå§‹åŒ–é…ç½®æ–‡ä»¶

# å®ç”¨å·¥å…·
flexllm pricing gpt-4     # æŸ¥è¯¢æ¨¡å‹å®šä»·
flexllm credits           # æŸ¥è¯¢ API Key ä½™é¢
flexllm mock              # å¯åŠ¨ Mock æœåŠ¡å™¨ï¼ˆæµ‹è¯•ç”¨ï¼‰
```

### é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®ï¼š`~/.flexllm/config.yaml`

```yaml
# é»˜è®¤æ¨¡å‹
default: "gpt-4"

# æ¨¡å‹åˆ—è¡¨
models:
  - id: gpt-4
    name: gpt-4
    provider: openai
    base_url: https://api.openai.com/v1
    api_key: your-api-key

  - id: local-ollama
    name: local-ollama
    provider: openai
    base_url: http://localhost:11434/v1
    api_key: EMPTY

# batch å‘½ä»¤é…ç½®ï¼ˆå¯é€‰ï¼‰
batch:
  concurrency: 20
  cache: true
  track_cost: true
```

ç¯å¢ƒå˜é‡ï¼ˆä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰ï¼š
- `FLEXLLM_BASE_URL` / `OPENAI_BASE_URL`
- `FLEXLLM_API_KEY` / `OPENAI_API_KEY`
- `FLEXLLM_MODEL` / `OPENAI_MODEL`

---

## æ¶æ„

```
flexllm/
â”œâ”€â”€ clients/           # æ‰€æœ‰å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ base.py        # æŠ½è±¡åŸºç±» (LLMClientBase)
â”‚   â”œâ”€â”€ llm.py         # ç»Ÿä¸€å…¥å£ (LLMClient)
â”‚   â”œâ”€â”€ openai.py      # OpenAI å…¼å®¹åç«¯
â”‚   â”œâ”€â”€ gemini.py      # Google Gemini åç«¯
â”‚   â”œâ”€â”€ claude.py      # Anthropic Claude åç«¯
â”‚   â”œâ”€â”€ pool.py        # å¤š Endpoint è´Ÿè½½å‡è¡¡
â”‚   â””â”€â”€ router.py      # Provider è·¯ç”±ç­–ç•¥
â”œâ”€â”€ pricing/           # æˆæœ¬ä¼°ç®—å’Œè¿½è¸ª
â”œâ”€â”€ cache/             # å“åº”ç¼“å­˜ (æ”¯æŒ IPC)
â”œâ”€â”€ async_api/         # é«˜æ€§èƒ½å¼‚æ­¥å¼•æ“
â””â”€â”€ msg_processors/    # å¤šæ¨¡æ€æ¶ˆæ¯å¤„ç†
```

åˆ†å±‚è®¾è®¡ï¼š

```
LLMClient (ç»Ÿä¸€å…¥å£ - æ¨èä½¿ç”¨)
    â”‚
    â”œâ”€â”€ Provider è‡ªåŠ¨è¯†åˆ«æˆ–æ˜¾å¼æŒ‡å®š
    â”‚
    â””â”€â”€ åç«¯å®¢æˆ·ç«¯ (å†…éƒ¨)
            â”œâ”€â”€ OpenAIClient
            â”œâ”€â”€ GeminiClient
            â””â”€â”€ ClaudeClient
                    â”‚
                    â””â”€â”€ LLMClientBase (æŠ½è±¡åŸºç±» - åªéœ€å®ç°4ä¸ªæ–¹æ³•)
                            â”‚
                            â”œâ”€â”€ ConcurrentRequester (å¼‚æ­¥å¼•æ“)
                            â”œâ”€â”€ ResponseCache (ç¼“å­˜å±‚)
                            â””â”€â”€ CostTracker (æˆæœ¬ç›‘æ§)
```

---

## è®¸å¯è¯

[Apache 2.0](LICENSE)
